{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 - Applied ML\n",
    "\n",
    "There are four aspects:\n",
    "* First, we process the data and explore the dataset (0).\n",
    "* Then, we classify labels. (1).\n",
    "* We plot learning curves (Bonus)\n",
    "* Finally, we do clustering (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Pre-Processing and Visualization\n",
    "Understanding the dataset, cleaning the variables. This is important because later we will need clean data for supervised and unsupervised learning.\n",
    "\n",
    "Using http://nbviewer.jupyter.org/github/mathewzilla/redcard/blob/master/Crowdstorming_visualisation.ipynb, we will clean the dataset. Then, we will aggregate information. Finally, we will clean up the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Tasks and Outline\n",
    "\n",
    "1) Dataset cleaning\n",
    "- exclude interactions by  ref who feature in fewer than 22 diyads\n",
    "\n",
    "- drop NAs with no race values, averaging race values\n",
    "\n",
    "2) Aggregate to player data\n",
    "\n",
    "- combine games, referee and bias information using variable statistics\n",
    "\n",
    "3) Cleaning features\n",
    "\n",
    "- take out unnecessary features â€” player ID, photo ID\n",
    "- turning date into proper format\n",
    "\n",
    "- convert categorical variables into dummy variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/CrowdstormingDataJuly1st.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dyads: 146028\n"
     ]
    }
   ],
   "source": [
    "orig_num_dyads = df.shape[0]\n",
    "print('Number of dyads:', orig_num_dyads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This line defines a new dataframe based on our >21 games filter\n",
    "all_refs = df.refNum.value_counts()\n",
    "good_refs = all_refs[all_refs>21]\n",
    "\n",
    "df=df[df['refNum'].isin(good_refs.index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of dyads left: 91.4215082039061\n"
     ]
    }
   ],
   "source": [
    "print('percentage of dyads left:', 100 * df.shape[0] / orig_num_dyads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Race Values\n",
    "\n",
    "Get the real \"race value\" by taking the average of the two. Then drop the ones that don't have rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['race_val'] = (df['rater1'] + df['rater2'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['race_val'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of dyads left: 77.9727175610157\n"
     ]
    }
   ],
   "source": [
    "# Percentage of data left...\n",
    "print('percentage of dyads left:', 100 * df.shape[0] / orig_num_dyads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping rater1, rater2, player, photo_ID and Alpha_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new.drop(['rater1','rater2','player','photoID','Alpha_3','birthday'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playerShort      False\n",
       "club             False\n",
       "leagueCountry    False\n",
       "height            True\n",
       "weight            True\n",
       "position          True\n",
       "games            False\n",
       "victories        False\n",
       "ties             False\n",
       "defeats          False\n",
       "goals            False\n",
       "yellowCards      False\n",
       "yellowReds       False\n",
       "redCards         False\n",
       "refNum           False\n",
       "refCountry       False\n",
       "meanIAT           True\n",
       "nIAT              True\n",
       "seIAT             True\n",
       "meanExp           True\n",
       "nExp              True\n",
       "seExp             True\n",
       "race_val         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1584,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.playerShort.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # Get the categorical values into a 2D numpy array\n",
    "train_categorical_values = np.array(df_new['playerShort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.,    44.,    63., ...,  1552.,  1560.,  1572.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_categorical_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# do the first column\n",
    "enc_label = LabelEncoder()\n",
    "train_data = enc_label.fit_transform(train_categorical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_categorical_values = train_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.,    44.,    63., ...,  1552.,  1560.,  1572.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_categorical_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nc_onehot = OneHotEncoder()\n",
    "train_cat_data = enc_label.fit_transform(train_categorical_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_cat_data_df = pd.DataFrame(np.asarray(train_cat_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113832</th>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113833</th>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113834</th>\n",
       "      <td>1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113835</th>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113836</th>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113837</th>\n",
       "      <td>1119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113838</th>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113839</th>\n",
       "      <td>1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113840</th>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113841</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113842</th>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113843</th>\n",
       "      <td>1259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113844</th>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113845</th>\n",
       "      <td>1291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113846</th>\n",
       "      <td>1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113847</th>\n",
       "      <td>1351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113848</th>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113849</th>\n",
       "      <td>1376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113850</th>\n",
       "      <td>1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113851</th>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113852</th>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113853</th>\n",
       "      <td>1437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113854</th>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113855</th>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113856</th>\n",
       "      <td>1494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113857</th>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113858</th>\n",
       "      <td>1546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113859</th>\n",
       "      <td>1552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113860</th>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113861</th>\n",
       "      <td>1572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113862 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0          0\n",
       "1         44\n",
       "2         63\n",
       "3         88\n",
       "4        102\n",
       "5        130\n",
       "6        150\n",
       "7        186\n",
       "8        276\n",
       "9        277\n",
       "10       347\n",
       "11       392\n",
       "12       531\n",
       "13       570\n",
       "14       628\n",
       "15       654\n",
       "16       657\n",
       "17       760\n",
       "18       857\n",
       "19       973\n",
       "20      1001\n",
       "21      1029\n",
       "22      1063\n",
       "23      1098\n",
       "24      1100\n",
       "25      1167\n",
       "26      1195\n",
       "27      1431\n",
       "28      1573\n",
       "29         6\n",
       "...      ...\n",
       "113832  1070\n",
       "113833  1089\n",
       "113834  1092\n",
       "113835  1095\n",
       "113836  1097\n",
       "113837  1119\n",
       "113838  1127\n",
       "113839  1197\n",
       "113840  1223\n",
       "113841  1225\n",
       "113842  1258\n",
       "113843  1259\n",
       "113844  1280\n",
       "113845  1291\n",
       "113846  1348\n",
       "113847  1351\n",
       "113848  1375\n",
       "113849  1376\n",
       "113850  1378\n",
       "113851  1381\n",
       "113852  1396\n",
       "113853  1437\n",
       "113854  1478\n",
       "113855  1486\n",
       "113856  1494\n",
       "113857  1500\n",
       "113858  1546\n",
       "113859  1552\n",
       "113860  1560\n",
       "113861  1572\n",
       "\n",
       "[113862 rows x 1 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification\n",
    "### Training Random Forest\n",
    "### Parameter Fitting\n",
    "### Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus \n",
    "Learning Curves: Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering\n",
    "### Feature Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff People have done/Discussions on Slack:\n",
    "\n",
    "## First Discussion -- Input features\n",
    "Ismail Bensouda Koraichi\t[7:48 PM]  \n",
    "Hello, I would like to clarify something. In the first question of the homework : \" [...] given a soccer player description outputs his skin color\", what does \"soccer player description\" mean ? I mean what should be the input ? A dyad ? An aggregation of multiple dyads ?\n",
    "\n",
    "Gael Lederrey\t[9:24 PM]  \n",
    "@ismail64 I think that we'll have to aggregate the data by players.. But then, we loose some information (the IAT and Exp for examples).. Therefore, it can be interesting to create a new feature taking into account the removed feature..\n",
    "\n",
    "But maybe there's another way to deal with the data without aggregating them.. =)\n",
    "\n",
    "Dunai Fuentes Hitos\t[11:31 PM]  \n",
    "I went with the aggregation by player.\n",
    "The \"smart\" way to deal with those features (IAT, Exp, etc.) would be to make a correction upon the number of yellow and red cards received by the player, as this is the only link players have to the referee...\n",
    "But it all looks a bit far-fetched, so I particularly decided to remove all this data under a presumption of honesty (the referee's honesty) (edited)\n",
    "\n",
    "Gael Lederrey\t[11:57 PM]  \n",
    "@dunai That's what I did too.. ^^ \n",
    "But I'm pretty sure racism exists everywhere. So I just created a weighted sum for the cards, the referees' \"racism\" values, and the number of times they encountered a player.. This gives you sort of a score linking the _IAT_ and the _Exp_ to all the referees who encounter a player..\n",
    "\n",
    "At the end, I'm pretty sure that it's important to keep these values.. Because the other values (goals, height, weight, games, etc.) can't be linked to the color of a player.. So, there's only these two \"racism\" values linked to the number of cards that can maybe give some information..\n",
    "\n",
    "If someone has another idea, I'd be glad to read about it.. =) (edited)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Discussion - Aggregating Features\n",
    "\n",
    "Jonas Racine\t[3:39 PM]  \n",
    "hey guys, is anyone able to get much more than ~70% accuracy on the homework? (exercise 1)\n",
    "\n",
    "bojan.petrovski\t[3:42 PM]  \n",
    "I got something like 78-80\n",
    "\n",
    "[3:43]  \n",
    "But I'm aggregating the results by player\n",
    "\n",
    "Gael Lederrey\t[3:53 PM]  \n",
    "@bojan.petrovski Did you create new features using the ones we loose (Like the IAT and Exp)?\n",
    "\n",
    "bojan.petrovski\t[4:14 PM]  \n",
    "Yeah for the IATmean and EXPmean I averaged them and for IATstd and EXPstd I calculated the Stadndard deviation on the samples that I used for the new means, because you can't just average standard deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Third Discussion - Dealing with Categorical Features\n",
    "\n",
    "Paul Nicolet\t[10:01 AM]  \n",
    "Hey guys Iâ€™m curious to know if you have a good way to deal with the categorical features in the homework, as `scikit-learn` doesnâ€™t accept them as strings. I figured out a way to encode them using the `OneHotEncoder()` class, in order to get a vector representation of each category, but I have some issues to deal with these vectors now, given a DataFrame doesnâ€™t accept vectors as valuesâ€¦ The easy way would be to encode them as integers but itâ€™s risky because it could be interpreted as continuous and ordered data right ?\n",
    "\n",
    "arnaudmiribel\t[10:05 AM]  \n",
    "Weâ€™ve used `LabelEncoder()` which naÃ¯vely binds labels to integers values. There are some issues doing this as two successive cells will be numerically _closer_ than two far apart cells (whereas there is no reason for this), so Iâ€™d also welcome any hint on this\n",
    "\n",
    "Baptiste Billardon\t[10:08 AM]  \n",
    "From what I read, there are no implementation of random forest in sklearn that handles categorical features. I also labelEncoded the categorical features then oneHotEncoded them\n",
    "\n",
    "Ondine Chanon\t[11:20 AM]  \n",
    "And what about dummy variables? It increases the number of attributes, but at least each cell has equal importance and there is no ordering issue.\n",
    "\n",
    "Ciprian Tomoiaga\t[3:53 PM]  \n",
    "@bojan.petrovski what would be the reason for averaging IATmean (or EXPmean) ?\n",
    "\n",
    "[3:55]  \n",
    "I mean, it's great that you get a good score, but it doesn't make much sense to me to average independent values. I mean, they are not related to the player, but to the referee\n",
    "\n",
    "Alexis Semple\t[4:25 PM]  \n",
    "Hey guys, I wonder how you understood the first sentence of the description for exercise 2 in this homework:\n",
    "> Aggregate the _referee information_ grouping by soccer player\n",
    "Maybe I'm missing something, but it seems to me that this indicates that we should not use features relevant only to the player himself (e.g. height, weight, position...) (edited)\n",
    "\n",
    "Gianrocco Lazzari\t[4:49 PM]  \n",
    "well, in principle I donâ€™t see what you should exclude themâ€¦itâ€™s about reducing the variability across referees (the way I understand it) - it might  be that  player-dependent-features are still relevant (edited)\n",
    "\n",
    "bojan.petrovski\t[5:19 PM]  \n",
    "@cipri_tom well in an ideal world the distribution of red and yellow cards should only depend on the position of the player, i.e. defenders are more likely to make a serious offence. So the cards alone should not help in any way to distinguish between black and white players. My idea is that some referees have a bias so the distribution is not the same for black and white players. And the average of the IAT just gives you a very crude idea of how much the player was discriminated against. For example if the average IAT is high and the player is black I would expect the distribution of cards for that player to be very different and if the IAT is low than you would expect the difference to be small in relation to the distribution for white players. I saw some people mention doing a weighted average based on the number of cards, but I think that in that way you are skewing the average because the lack of a card is also valuable information. (edited)\n",
    "\n",
    "[5:20]  \n",
    "But again it's hard to tell if the model is actually doing this kind of prediction or is picking up on some other factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
