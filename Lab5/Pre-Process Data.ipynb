{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "d = os.getcwd()\n",
    "\n",
    "email_text = open(path.join(d, 'emails.txt')).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I found this code online, it does everything, except I can't debug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing text and html (Tokenizing words and sentences, clean HTML, clean text, removing stopwords, stemming and lemmatization)\n",
    "__author__ : Triskelion user@Kaggle (Thanks: Abhishek Thakur & Foxtrot user@Kaggle)\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from nltk import clean_html\n",
    "from nltk import SnowballStemmer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Tokenizing (Document to list of sentences. Sentence to list of words.)\n",
    "def tokenize(str):\n",
    "\t'''Tokenizes into sentences, then strips punctuation/abbr, converts to lowercase and tokenizes words'''\n",
    "\treturn \t[word_tokenize(\" \".join(re.findall(r'\\w+', t,flags = re.UNICODE | re.LOCALE)).lower()) \n",
    "\t\t\tfor t in sent_tokenize(str.replace(\"'\", \"\"))]\n",
    "\n",
    "#Removing stopwords. Takes list of words, outputs list of words.\n",
    "def remove_stopwords(l_words, lang='english'):\n",
    "\tl_stopwords = stopwords.words(lang)\n",
    "\tcontent = [w for w in l_words if w.lower() not in l_stopwords]\n",
    "\treturn content\n",
    "\t\t\n",
    "\t\t\n",
    "#Stem all words with stemmer of type, return encoded as \"encoding\"\n",
    "def stemming(words_l, type=\"PorterStemmer\", lang=\"english\", encoding=\"utf8\"):\n",
    "\tsupported_stemmers = [\"PorterStemmer\",\"SnowballStemmer\",\"LancasterStemmer\",\"WordNetLemmatizer\"]\n",
    "\tif type is False or type not in supported_stemmers:\n",
    "\t\treturn words_l\n",
    "\telse:\n",
    "\t\tl = []\n",
    "\t\tif type == \"PorterStemmer\":\n",
    "\t\t\tstemmer = PorterStemmer()\n",
    "\t\t\tfor word in words_l:\n",
    "\t\t\t\tl.append(stemmer.stem(word).encode(encoding))\n",
    "\t\tif type == \"SnowballStemmer\":\n",
    "\t\t\tstemmer = SnowballStemmer(lang)\n",
    "\t\t\tfor word in words_l:\n",
    "\t\t\t\tl.append(stemmer.stem(word).encode(encoding))\n",
    "\t\tif type == \"LancasterStemmer\":\n",
    "\t\t\tstemmer = LancasterStemmer()\n",
    "\t\t\tfor word in words_l:\n",
    "\t\t\t\tl.append(stemmer.stem(word).encode(encoding))\n",
    "\t\tif type == \"WordNetLemmatizer\": #TODO: context\n",
    "\t\t\twnl = WordNetLemmatizer()\n",
    "\t\t\tfor word in words_l:\n",
    "\t\t\t\tl.append(wnl.lemmatize(word).encode(encoding))\n",
    "\t\treturn l\n",
    "\n",
    "#The preprocess pipeline. Returns as lists of tokens or as string. If stemmer_type = False or not supported then no stemming.\t\t\n",
    "def preprocess_pipeline(str, lang=\"english\", stemmer_type=\"PorterStemmer\", return_as_str=False, \n",
    "\t\t\t\t\t\tdo_remove_stopwords=False, do_clean_html=False):\n",
    "\tl = []\n",
    "\twords = []\n",
    "\tsentences = tokenize(str)\n",
    "\tfor sentence in sentences:\n",
    "\t\tif do_remove_stopwords:\n",
    "\t\t\twords = remove_stopwords(sentence, lang)\n",
    "\t\telse:\n",
    "\t\t\twords = sentence\n",
    "\t\twords = stemming(words, stemmer_type)\n",
    "\t\tprint(words)\n",
    "\t\twords = [word.decode(\"utf-8\") for word in words]\n",
    "\t\tprint(words)\n",
    "\t\tif return_as_str:\n",
    "\t\t\tl.append(\" \".join(words))\n",
    "\t\telse:\n",
    "\t\t\tl.append(words)\n",
    "\tif return_as_str:\n",
    "\t\treturn \" \".join(l)\n",
    "\telse:\n",
    "\t\treturn l\n",
    "\n",
    "#test_sentence = \"User-Testing Tester Tests! She had me at 'hello'?!? But then <abbr>ESPN</abbr> fainted... and Eighty cars drove past.\"\n",
    "#print \"\\nOriginal:\\n\", test_sentence\n",
    "#print \"\\nPorter:\\n\", preprocess_pipeline(test_sentence, \"english\", \"PorterStemmer\", True, False, True)\n",
    "#print \"\\nLancaster:\\n\", preprocess_pipeline(test_sentence, \"english\", \"LancasterStemmer\", True, False, True)\n",
    "#print \"\\nWordNet:\\n\", preprocess_pipeline(test_sentence, \"english\", \"WordNetLemmatizer\", True, False, True)\n",
    "#print \"\\nStopword Tokenized Lancaster:\\n\", preprocess_pipeline(test_sentence, \"english\", \"LancasterStemmer\", False, True, True)\n",
    "#print \"\\nOnly cleaning (HTML+Text):\\n\", preprocess_pipeline(test_sentence, \"english\", False, True, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the result of the pre-process pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original:\n",
      " User-Testing Tester Tests! She had me at 'hello'?!? But then fainted... and Eighty cars drove past.\n",
      "[b'user', b'test', b'tester', b'test']\n",
      "['user', 'test', 'tester', 'test']\n",
      "[b'she', b'had', b'me', b'at', b'hello']\n",
      "['she', 'had', 'me', 'at', 'hello']\n",
      "[b'but', b'then', b'faint', b'and', b'eighti', b'car', b'drove', b'past']\n",
      "['but', 'then', 'faint', 'and', 'eighti', 'car', 'drove', 'past']\n",
      "\n",
      "Porter:\n",
      " user test tester test she had me at hello but then faint and eighti car drove past\n",
      "[b'us', b'test', b'test', b'test']\n",
      "['us', 'test', 'test', 'test']\n",
      "[b'she', b'had', b'me', b'at', b'hello']\n",
      "['she', 'had', 'me', 'at', 'hello']\n",
      "[b'but', b'then', b'faint', b'and', b'eighty', b'car', b'drov', b'past']\n",
      "['but', 'then', 'faint', 'and', 'eighty', 'car', 'drov', 'past']\n",
      "\n",
      "Lancaster:\n",
      " [['us', 'test', 'test', 'test'], ['she', 'had', 'me', 'at', 'hello'], ['but', 'then', 'faint', 'and', 'eighty', 'car', 'drov', 'past']]\n",
      "[b'user', b'testing', b'tester', b'test']\n",
      "['user', 'testing', 'tester', 'test']\n",
      "[b'she', b'had', b'me', b'at', b'hello']\n",
      "['she', 'had', 'me', 'at', 'hello']\n",
      "[b'but', b'then', b'fainted', b'and', b'eighty', b'car', b'drove', b'past']\n",
      "['but', 'then', 'fainted', 'and', 'eighty', 'car', 'drove', 'past']\n",
      "\n",
      "WordNet:\n",
      " [['user', 'testing', 'tester', 'test'], ['she', 'had', 'me', 'at', 'hello'], ['but', 'then', 'fainted', 'and', 'eighty', 'car', 'drove', 'past']]\n",
      "[b'us', b'test', b'test', b'test']\n",
      "['us', 'test', 'test', 'test']\n",
      "[b'hello']\n",
      "['hello']\n",
      "[b'faint', b'eighty', b'car', b'drov', b'past']\n",
      "['faint', 'eighty', 'car', 'drov', 'past']\n",
      "\n",
      "Stopword Tokenized Lancaster:\n",
      " [['us', 'test', 'test', 'test'], ['hello'], ['faint', 'eighty', 'car', 'drov', 'past']]\n",
      "['user', 'testing', 'tester', 'tests']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-adb3479c1945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nWordNet:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WordNetLemmatizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStopword Tokenized Lancaster:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LancasterStemmer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nOnly cleaning (HTML+Text):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-a8eaa82d12e1>\u001b[0m in \u001b[0;36mpreprocess_pipeline\u001b[0;34m(str, lang, stemmer_type, return_as_str, do_remove_stopwords, do_clean_html)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmer_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreturn_as_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-a8eaa82d12e1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmer_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreturn_as_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "test_sentence = \"User-Testing Tester Tests! She had me at 'hello'?!? But then fainted... and Eighty cars drove past.\"\n",
    "print(\"\\nOriginal:\\n\", test_sentence)\n",
    "print (\"\\nPorter:\\n\", preprocess_pipeline(test_sentence, \"english\", \"PorterStemmer\", True, False, True))\n",
    "print (\"\\nLancaster:\\n\", preprocess_pipeline(test_sentence, \"english\", \"LancasterStemmer\", False, False, True))\n",
    "print (\"\\nWordNet:\\n\", preprocess_pipeline(test_sentence, \"english\", \"WordNetLemmatizer\", False, False, True))\n",
    "print (\"\\nStopword Tokenized Lancaster:\\n\", preprocess_pipeline(test_sentence, \"english\", \"LancasterStemmer\", False, True, True))\n",
    "print (\"\\nOnly cleaning (HTML+Text):\\n\", preprocess_pipeline(test_sentence, \"english\", False, True, False, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ that's the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "words = []\n",
    "sentences = tokenize(email_text)\n",
    "for sentence in sentences:\n",
    "    words = remove_stopwords(sentence, lang='english')\n",
    "    words = stemming(words, type=\"SnowballStemmer\") #[\"PorterStemmer\",\"SnowballStemmer\",\"LancasterStemmer\",\"WordNetLemmatizer\"]\n",
    "    l.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding customized stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = [b'user', b'test', b'tester', b'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [word.decode(\"utf-8\") for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user', 'test', 'tester', 'test']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in l[1]:\n",
    "    word = word.decode(\"utf-8\")\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [word.decode(\"utf-8\") for word in l[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', 'pis', 'print']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = b'5'.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-4389930cb909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Open a plot of the generated image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tbfang/anaconda3/lib/python3.5/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \"\"\"\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tbfang/anaconda3/lib/python3.5/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \"\"\"\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tbfang/anaconda3/lib/python3.5/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    384\u001b[0m         flags = (re.UNICODE if sys.version < '3' and type(text) is unicode\n\u001b[1;32m    385\u001b[0m                  else 0)\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\w[\\w']+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tbfang/anaconda3/lib/python3.5/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(max_words=4000).generate(l)\n",
    "# Open a plot of the generated image.\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'us', b'test', b'test', b'test'],\n",
       " [b'hello'],\n",
       " [b'faint', b'eighty', b'car', b'drov', b'past']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "words = []\n",
    "sentences = tokenize(test_sentence)\n",
    "for sentence in sentences:\n",
    "    words = remove_stopwords(sentence, lang='english')\n",
    "    words = stemming(words, type=\"LancasterStemmer\") #[\"PorterStemmer\",\"SnowballStemmer\",\"LancasterStemmer\",\"WordNetLemmatizer\"]\n",
    "    l.append(words)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'user', b'testing', b'tester', b'test'],\n",
       " [b'hello'],\n",
       " [b'fainted', b'eighty', b'car', b'drove', b'past']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "words = []\n",
    "sentences = tokenize(test_sentence)\n",
    "for sentence in sentences:\n",
    "    words = remove_stopwords(sentence, lang='english')\n",
    "    words = stemming(words, type=\"WordNetLemmatizer\") #[\"PorterStemmer\",\"SnowballStemmer\",\"LancasterStemmer\",\"WordNetLemmatizer\"]\n",
    "    l.append(words)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['user', 'testing', 'tester', 'tests'],\n",
       " ['she', 'had', 'me', 'at', 'hello'],\n",
       " ['but', 'then', 'fainted', 'and', 'eighty', 'cars', 'drove', 'past']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
